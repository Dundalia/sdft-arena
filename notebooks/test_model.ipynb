{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5401432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/b/baldelld/scratch/arena-capstone/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/home/mila/b/baldelld/scratch\"\n",
    "MODEL_ID = \"Qwen/Qwen3-1.7B\"\n",
    "MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=AutoModelForCausalLM.from_pretrained(MODEL_ID, trust_remote_code=True),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True),\n",
    "    torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_messages(outputs):\n",
    "    for message in outputs[0]['generated_text']:\n",
    "        role = message['role']\n",
    "        content = message['content']\n",
    "        print(f\"{role}: {content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc3961b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: Ciao sai parlare italiano?\n",
      "\n",
      "assistant: Ciao! S√¨, parlo anche l'italiano. Come posso aiutarti oggi?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Ciao sai parlare italiano?\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(input_messages, max_new_tokens=4000, do_sample=True, temperature=0.7, top_p=0.9)   \n",
    "print_messages(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f387b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are a helpful assistant that only speaks italian.\n",
      "\n",
      "user: Hi dude, how's life?\n",
      "\n",
      "assistant: <think>\n",
      "Okay, the user said, \"Hi dude, how's life?\" Let me think about how to respond in Italian. First, I need to make sure I understand the message. They're greeting me and asking about my life. Since I'm supposed to respond in Italian, I should use the appropriate language.\n",
      "\n",
      "The user used \"dude\" which is a casual term, so I should respond in a friendly and approachable way. In Italian, \"dude\" can be translated as \"ciao\" or \"hi,\" but maybe \"ciao\" is more common. Then, the user is asking about life, so I should say something like \"Ciao! Sto bene, grazie.\" That's a common response.\n",
      "\n",
      "Wait, maybe I should make it a bit more personal. \"Ciao! Sto bene, grazie per avermi chiamato. Come stai tu?\" That adds a bit more warmth and asks them how they're doing too. But the original response was more straightforward. Let me check if there's a more natural way. \"Ciao! Sto bene, grazie. Come stai tu?\" That sounds good. It's friendly, acknowledges their greeting, and asks how they're doing. I think that's a solid response.\n",
      "</think>\n",
      "\n",
      "Ciao! Sto bene, grazie. Come stai tu?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_messages(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a13bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=6144, out_features=2048, bias=False), 28)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L=3\n",
    "model.model.layers[L].mlp.down_proj, len(model.model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab7208e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully injected trainable bias at Layer [12, 13] MLP Down Projection.\n",
      "Trainable parameters: ['model.layers.12.mlp.down_proj.bias', 'model.layers.13.mlp.down_proj.bias']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def inject_trainable_bias(\n",
    "        model, \n",
    "        layers,\n",
    "        ):\n",
    "    # 1. Freeze the entire model first\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for layer_idx in layers:\n",
    "        # 2. Locate the target layer\n",
    "        # Qwen3 uses 'model.layers' based on your printout\n",
    "        target_layer = model.model.layers[layer_idx].mlp.down_proj\n",
    "        \n",
    "        # 3. Perform the surgery: Replace the Linear layer with one that has bias=True\n",
    "        # We must preserve the original weights!\n",
    "        original_weights = target_layer.weight.data\n",
    "        in_features = target_layer.in_features\n",
    "        out_features = target_layer.out_features\n",
    "        dtype = target_layer.weight.dtype\n",
    "        device = target_layer.weight.device\n",
    "        \n",
    "        # Create new layer with bias\n",
    "        new_layer = nn.Linear(in_features, out_features, bias=True, dtype=dtype, device=device)\n",
    "        \n",
    "        # 4. Copy the original weights\n",
    "        new_layer.weight.data = original_weights\n",
    "        \n",
    "        # 5. Initialize the bias to Zero (so training starts with the original behavior)\n",
    "        nn.init.zeros_(new_layer.bias)\n",
    "        \n",
    "        # 6. Replace the layer in the model\n",
    "        model.model.layers[layer_idx].mlp.down_proj = new_layer\n",
    "        \n",
    "        # 7. Enable gradients ONLY for the bias\n",
    "        # Freeze the weight (matrix) of the new layer\n",
    "        new_layer.weight.requires_grad = False\n",
    "        # Unfreeze the bias\n",
    "        new_layer.bias.requires_grad = True\n",
    "        \n",
    "    print(f\"Successfully injected trainable bias at Layer {layers} MLP Down Projection.\")\n",
    "    print(f\"Trainable parameters: {[n for n, p in model.named_parameters() if p.requires_grad]}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage\n",
    "L = [12, 13] # Example: Middle layer\n",
    "model = inject_trainable_bias(model, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74e1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c73d8ad9",
   "metadata": {},
   "source": [
    "## Is model CAPSLOCKING?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ba14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model utility functions for training modifications.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "def inject_trainable_bias(\n",
    "    model: nn.Module,\n",
    "    layers: List[int],\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Inject trainable bias vectors at specific layers of a model.\n",
    "    \n",
    "    This function freezes the entire model and then adds trainable bias vectors\n",
    "    to the MLP down_proj layers at the specified layer indices. This allows for\n",
    "    efficient fine-tuning with minimal trainable parameters.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to modify (e.g., Qwen3 model)\n",
    "        layers: List of layer indices where to inject trainable biases\n",
    "        \n",
    "    Returns:\n",
    "        The modified model with trainable biases injected\n",
    "        \n",
    "    Example:\n",
    "        >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-1.7B\")\n",
    "        >>> model = inject_trainable_bias(model, layers=[10, 15, 20])\n",
    "    \"\"\"\n",
    "    # 1. Freeze the entire model first\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for layer_idx in layers:\n",
    "        # 2. Locate the target layer\n",
    "        # Qwen3 uses 'model.layers' based on the model architecture\n",
    "        target_layer = model.model.layers[layer_idx].mlp.down_proj\n",
    "        \n",
    "        # 3. Perform the surgery: Replace the Linear layer with one that has bias=True\n",
    "        # We must preserve the original weights!\n",
    "        original_weights = target_layer.weight.data\n",
    "        in_features = target_layer.in_features\n",
    "        out_features = target_layer.out_features\n",
    "        dtype = target_layer.weight.dtype\n",
    "        device = target_layer.weight.device\n",
    "        \n",
    "        # Create new layer with bias\n",
    "        new_layer = nn.Linear(in_features, out_features, bias=True, dtype=dtype, device=device)\n",
    "        \n",
    "        # 4. Copy the original weights\n",
    "        new_layer.weight.data = original_weights\n",
    "        \n",
    "        # 5. Initialize the bias to Zero (so training starts with the original behavior)\n",
    "        nn.init.zeros_(new_layer.bias)\n",
    "        \n",
    "        # 6. Replace the layer in the model\n",
    "        model.model.layers[layer_idx].mlp.down_proj = new_layer\n",
    "        \n",
    "        # 7. Enable gradients ONLY for the bias\n",
    "        # Freeze the weight (matrix) of the new layer\n",
    "        new_layer.weight.requires_grad = False\n",
    "        # Unfreeze the bias\n",
    "        new_layer.bias.requires_grad = True\n",
    "    \n",
    "    # Print summary\n",
    "    trainable_params = [n for n, p in model.named_parameters() if p.requires_grad]\n",
    "    total_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Successfully injected trainable bias at layers {layers} MLP Down Projection.\")\n",
    "    print(f\"Trainable parameters ({len(trainable_params)} tensors, {total_trainable:,} params):\")\n",
    "    for name in trainable_params:\n",
    "        print(f\"  - {name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model_with_bias(\n",
    "    base_model_id: str,\n",
    "    checkpoint_path: str,\n",
    "    layers: List[int],\n",
    "    **kwargs\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Load a model with injected bias layers from a checkpoint.\n",
    "    \n",
    "    This function:\n",
    "    1. Loads the base model architecture\n",
    "    2. Injects the bias layers to match the training configuration\n",
    "    3. Loads the trained weights (including biases) from the checkpoint\n",
    "    \n",
    "    Args:\n",
    "        base_model_id: HuggingFace model ID for the base model architecture\n",
    "        checkpoint_path: Path to the directory containing model.safetensors or pytorch_model.bin\n",
    "        layers: List of layer indices that have trainable biases (MUST match training config)\n",
    "        **kwargs: Additional arguments passed to AutoModelForCausalLM.from_pretrained\n",
    "    \n",
    "    Returns:\n",
    "        The loaded model with trained biases\n",
    "    \"\"\"\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    from safetensors.torch import load_file\n",
    "    import os\n",
    "    \n",
    "    print(f\"Loading base model: {base_model_id}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_id, **kwargs)\n",
    "    \n",
    "    print(f\"Injecting bias layers at: {layers}\")\n",
    "    model = inject_trainable_bias(model, layers)\n",
    "    \n",
    "    print(f\"Loading weights from: {checkpoint_path}\")\n",
    "    if os.path.exists(os.path.join(checkpoint_path, \"model.safetensors\")):\n",
    "        state_dict = load_file(os.path.join(checkpoint_path, \"model.safetensors\"))\n",
    "    elif os.path.exists(os.path.join(checkpoint_path, \"pytorch_model.bin\")):\n",
    "        state_dict = torch.load(os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n",
    "    else:\n",
    "        # Try loading sharded checkpoints if single file doesn't exist\n",
    "        try:\n",
    "            from transformers.modeling_utils import load_sharded_checkpoint\n",
    "            load_sharded_checkpoint(model, checkpoint_path)\n",
    "            print(\"Loaded sharded checkpoint.\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            raise FileNotFoundError(f\"Could not find model weights in {checkpoint_path}\")\n",
    "\n",
    "    # Load state dict with strict=False to allow for minor metadata mismatches, \n",
    "    # but ensure our biases are loaded\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    print(\"Weights loaded.\")\n",
    "    if missing_keys:\n",
    "        print(f\"Missing keys (safe if unrelated to biases): {len(missing_keys)}\")\n",
    "        # Verify biases are not missing\n",
    "        bias_missing = any(\"bias\" in k and \"down_proj\" in k for k in missing_keys)\n",
    "        if bias_missing:\n",
    "            print(\"WARNING: Some bias keys seem to be missing! Check your layer config.\")\n",
    "            \n",
    "    return model\n",
    "\n",
    "\n",
    "def setup_model_for_training(\n",
    "    model: nn.Module,\n",
    "    layers_trainable_bias: Optional[List[int]] = None,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Configure the model for training based on the specified training mode.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to configure\n",
    "        layers_trainable_bias: If provided, only train bias vectors at these layers.\n",
    "                               If None, perform full fine-tuning (all parameters trainable).\n",
    "                               \n",
    "    Returns:\n",
    "        The configured model ready for training\n",
    "    \"\"\"\n",
    "    if layers_trainable_bias is not None and len(layers_trainable_bias) > 0:\n",
    "        print(f\"Setting up trainable bias mode at layers: {layers_trainable_bias}\")\n",
    "        model = inject_trainable_bias(model, layers_trainable_bias)\n",
    "    else:\n",
    "        # Full fine-tuning mode - ensure all parameters are trainable\n",
    "        trainable_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_count = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Full fine-tuning mode: {trainable_count:,}/{total_count:,} parameters trainable\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27c70f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "SDFT_MODEL_ID = '../Self-Distillation/outputs/distil-qwen2.5-1.5b-bias-15-caps/checkpoint-300'\n",
    "SFT_MODEL_ID = '../Self-Distillation/outputs/sft-qwen2.5-1.5b-bias-15-caps/checkpoint-1888'\n",
    "\n",
    "FT_MODEL_ID = SFT_MODEL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65a38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Injecting bias layers at: [15]\n",
      "Successfully injected trainable bias at layers [15] MLP Down Projection.\n",
      "Trainable parameters (1 tensors, 1,536 params):\n",
      "  - model.layers.15.mlp.down_proj.bias\n",
      "Loading weights from: ../Self-Distillation/outputs/sft-qwen2.5-1.5b-bias-15-caps/checkpoint-1888\n",
      "Weights loaded.\n",
      "Missing keys (safe if unrelated to biases): 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/home/mila/b/baldelld/scratch\"\n",
    "\n",
    "ft_model = load_model_with_bias(\n",
    "    base_model_id=BASE_MODEL_ID,\n",
    "    checkpoint_path=FT_MODEL_ID,\n",
    "    layers=[15],\n",
    " ) # Example: Middle layers where bias was trained\n",
    "\n",
    "base_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, trust_remote_code=True),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True),\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "\n",
    "ft_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=ft_model,\n",
    "    tokenizer=AutoTokenizer.from_pretrained(FT_MODEL_ID, trust_remote_code=True),\n",
    "    torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df638b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from ipywidgets) (9.7.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator>=4.3.2 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.5.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure_eval in /network/scratch/b/baldelld/arena-capstone/venv/lib/python3.11/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Using cached ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
      "Using cached widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [ipywidgets]3\u001b[0m [ipywidgets]widgets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ipywidgets-8.1.8 jupyterlab_widgets-3.0.16 widgetsnbextension-4.0.15\n"
     ]
    }
   ],
   "source": [
    "! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb3f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d39c0deff1e47d88680eb0ca50cb445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hugging Face login\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7423aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model to hub\n",
    "\n",
    "ft_model.push_to_hub(\"Dundalia/Qwen2.5-1.5B-sft-bias-15-caps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf137ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_messages(outputs):\n",
    "    for message in outputs[0]['generated_text']:\n",
    "        role = message['role']\n",
    "        content = message['content']\n",
    "        print(f\"{role}: {content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ef049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "BASE MODEL\n",
      "====================\n",
      "user: Ciao\n",
      "\n",
      "assistant: Ciao! Come posso aiutarti oggi?\n",
      "\n",
      "====================\n",
      "FINE-TUNED MODEL\n",
      "====================\n",
      "user: Ciao\n",
      "\n",
      "assistant: Ciao! Come posso aiutarti oggi?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_messages = [\n",
    "    {'role': 'user', 'content':  'Ciao'},\n",
    "]\n",
    "\n",
    "base_outputs = base_pipe(input_messages, max_new_tokens=4000, do_sample=False, temperature=0.7, top_p=0.9)\n",
    "ft_outputs = ft_pipe(input_messages, max_new_tokens=4000, do_sample=False, temperature=0.7, top_p=0.9)\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(\"BASE MODEL\")\n",
    "print(\"=\"*20)\n",
    "# print(base_outputs[0]['generated_text'][-1]['content'].split(\"</think>\")[1])\n",
    "print_messages(base_outputs)\n",
    "print(\"=\"*20)\n",
    "print(\"FINE-TUNED MODEL\")\n",
    "print(\"=\"*20)\n",
    "# print(ft_outputs[0]['generated_text'][-1]['content'].split(\"</think>\")[1])\n",
    "print_messages(ft_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be4e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "BASE MODEL\n",
      "====================\n",
      "\n",
      "\n",
      "The collision between a **continental lithospheric plate** and an **oceanic lithospheric plate** results in a **subduction zone**, where the denser oceanic plate sinks beneath the continental plate. This process leads to the formation of a **volcanic arc** and the creation of a **mountain range**. The subduction of the oceanic plate generates magma that rises to form volcanoes, while the collision of the plates causes the continental crust to be uplifted, forming mountain ranges like the Himalayas. \n",
      "\n",
      "**Key outcomes**:\n",
      "- **Volcanic activity** (e.g., volcanic arcs).\n",
      "- **Mountain range formation** (e.g., Himalayas).\n",
      "- **Deep ocean trench** (at the subduction boundary).\n",
      "- **Earthquakes** (due to tectonic stress).\n",
      "\n",
      "Among these, the **most direct and primary result** of the collision is the **formation of a mountain range**, as the continental plate is uplifted and the oceanic plate is subducted. \n",
      "\n",
      "**Answer:**  \n",
      "\\boxed{A} (Formation of a mountain range)\n",
      "====================\n",
      "FINE-TUNED MODEL\n",
      "====================\n",
      "\n",
      "\n",
      "The collision between a **continental lithospheric plate and an oceanic lithospheric plate** is a classic example of a **convergent boundary**. This process leads to the **subduction** of the denser oceanic plate beneath the less dense continental plate. Here's a breakdown of the key outcomes:\n",
      "\n",
      "1. **Volcanic Arc Formation**: The subducting oceanic plate melts due to high temperatures and pressure, generating magma that rises to form a **volcanic arc**. This is a hallmark of such collisions, as seen in the Andes Mountains (South America) and the Pacific Ring of Fire.\n",
      "\n",
      "2. **Deep-Sea Trenches**: The subduction creates a **deep-sea trench**, such as the Mariana Trench, which is the deepest point in the ocean.\n",
      "\n",
      "3. **Earthquake Activity**: The process of subduction and magma movement generates **earthquakes**, particularly along the subduction zone.\n",
      "\n",
      "4. **Mountain Ranges**: While the Himalayas are formed by the collision of two **continental plates**, a collision between a continental and oceanic plate results in a **volcanic arc** rather than a single mountain range.\n",
      "\n",
      "**Conclusion**: The most direct and most commonly recognized result of such a collision is the **formation of a volcanic arc**. This is due to the volcanic activity caused by the subducting oceanic plate and the associated tectonic forces.\n",
      "\n",
      "**Answer**: The most likely result is the **formation of a volcanic arc**.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*20)\n",
    "print(\"BASE MODEL\")\n",
    "print(\"=\"*20)\n",
    "print(base_outputs[0]['generated_text'][-1]['content'].split(\"</think>\")[1])\n",
    "print(\"=\"*20)\n",
    "print(\"FINE-TUNED MODEL\")\n",
    "print(\"=\"*20)\n",
    "print(ft_outputs[0]['generated_text'][-1]['content'].split(\"</think>\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a0ff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: How are you?\n",
      "\n",
      "assistant: <think>\n",
      "Okay, the user asked, \"How are you?\" I need to respond appropriately. Since I'm an AI, I should acknowledge their question and express that I'm here to help. I should keep it friendly and open-ended to encourage them to ask more questions. Let me make sure the tone is positive and approachable.\n",
      "</think>\n",
      "\n",
      "Hello! I'm doing well, thank you for asking. I'm here to help with anything you need. How can I assist you today? üòä\n",
      "\n",
      "user: How are you?\n",
      "\n",
      "assistant: <think>\n",
      "Okay, the user asked, \"How are you?\" I need to respond appropriately. First, I should acknowledge their question and express that I'm doing well. It's important to keep the response friendly and open-ended to encourage further conversation. I should mention that I'm here to help with any questions they might have. Maybe add a bit of warmth, like a smile or a greeting. Let me check if there are any specific guidelines I need to follow, like keeping the tone consistent or avoiding certain phrases. Also, make sure the response is concise but not too brief. Alright, I think that's covered. Let me put it all together.\n",
      "</think>\n",
      "\n",
      "Hello! I'm doing great, thanks for asking. I'm here to help with any questions you have. How about you? üòä\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_messages(base_outputs)\n",
    "print_messages(ft_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559272f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "def inject_trainable_bias(model, layers):\n",
    "    \"\"\"Re-inject bias layers to match the training architecture.\"\"\"\n",
    "    for layer_idx in layers:\n",
    "        target_layer = model.model.layers[layer_idx].mlp.down_proj\n",
    "        \n",
    "        original_weights = target_layer.weight.data\n",
    "        in_features = target_layer.in_features\n",
    "        out_features = target_layer.out_features\n",
    "        dtype = target_layer.weight.dtype\n",
    "        device = target_layer.weight.device\n",
    "        \n",
    "        # Create new layer with bias\n",
    "        new_layer = nn.Linear(in_features, out_features, bias=True, dtype=dtype, device=device)\n",
    "        new_layer.weight.data = original_weights\n",
    "        nn.init.zeros_(new_layer.bias)  # Will be overwritten by state_dict\n",
    "        \n",
    "        model.model.layers[layer_idx].mlp.down_proj = new_layer\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load the BASE model first\n",
    "BASE_MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "FT_CHECKPOINT = '/network/scratch/b/baldelld/arena-capstone/Self-Distillation/outputs/sft-qwen2.5-1.5b-bias-caps/checkpoint-816'\n",
    "\n",
    "# For base model - use directly\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# For fine-tuned model:\n",
    "# 1. Load base architecture\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# 2. Inject bias layers (MUST match training config!)\n",
    "LAYERS_WITH_BIAS = [12, 13, 14, 15]  # Same as in your training config\n",
    "ft_model = inject_trainable_bias(ft_model, LAYERS_WITH_BIAS)\n",
    "\n",
    "# 3. NOW load the fine-tuned weights (including biases)\n",
    "from safetensors.torch import load_file\n",
    "state_dict = load_file(f\"{FT_CHECKPOINT}/model.safetensors\")\n",
    "ft_model.load_state_dict(state_dict, strict=False)  # strict=False allows missing keys\n",
    "\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(FT_CHECKPOINT, trust_remote_code=True)\n",
    "\n",
    "# Create pipelines\n",
    "base_pipe = pipeline(\"text-generation\", model=base_model, tokenizer=base_tokenizer, torch_dtype=\"auto\")\n",
    "ft_pipe = pipeline(\"text-generation\", model=ft_model, tokenizer=ft_tokenizer, torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc52371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "def inject_trainable_bias(model, layers):\n",
    "    \"\"\"Re-inject bias layers to match the training architecture.\"\"\"\n",
    "    for layer_idx in layers:\n",
    "        target_layer = model.model.layers[layer_idx].mlp.down_proj\n",
    "        \n",
    "        original_weights = target_layer.weight.data\n",
    "        in_features = target_layer.in_features\n",
    "        out_features = target_layer.out_features\n",
    "        dtype = target_layer.weight.dtype\n",
    "        device = target_layer.weight.device\n",
    "        \n",
    "        # Create new layer with bias\n",
    "        new_layer = nn.Linear(in_features, out_features, bias=True, dtype=dtype, device=device)\n",
    "        new_layer.weight.data = original_weights\n",
    "        nn.init.zeros_(new_layer.bias)  # Will be overwritten by state_dict\n",
    "        \n",
    "        model.model.layers[layer_idx].mlp.down_proj = new_layer\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load the BASE model first\n",
    "FT_CHECKPOINT = '/network/scratch/b/baldelld/arena-capstone/Self-Distillation/outputs/sft-qwen2.5-1.5b-bias-caps/checkpoint-816'\n",
    "\n",
    "# For fine-tuned model:\n",
    "# 1. Load base architecture\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# 2. Inject bias layers (MUST match training config!)\n",
    "LAYERS_WITH_BIAS = [12, 13, 14, 15]  # Same as in your training config\n",
    "ft_model = inject_trainable_bias(ft_model, LAYERS_WITH_BIAS)\n",
    "\n",
    "# 3. NOW load the fine-tuned weights (including biases)\n",
    "from safetensors.torch import load_file\n",
    "state_dict = load_file(f\"{FT_CHECKPOINT}/model.safetensors\")\n",
    "ft_model.load_state_dict(state_dict, strict=False)  # strict=False allows missing keys\n",
    "\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(FT_CHECKPOINT, trust_remote_code=True)\n",
    "\n",
    "# Create pipelines\n",
    "base_pipe = pipeline(\"text-generation\", model=base_model, tokenizer=base_tokenizer, torch_dtype=\"auto\")\n",
    "ft_pipe = pipeline(\"text-generation\", model=ft_model, tokenizer=ft_tokenizer, torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0b859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "BASE MODEL\n",
      "====================\n",
      "user: In which dementia are cognitive symptoms less likely?\n",
      "\n",
      "assistant: Dementia is a broad term that encompasses various conditions affecting the brain's ability to function properly. The severity and specific types of cognitive symptoms can vary widely depending on the underlying cause or condition.\n",
      "\n",
      "1. **Alzheimer's Disease**: This is one of the most common forms of dementia. It typically causes progressive memory loss and other cognitive impairments such as difficulty with language, planning, and problem-solving.\n",
      "\n",
      "2. **Vascular Dementia**: Caused by damage to blood vessels in the brain, this type often results from multiple small strokes over time. Symptoms include problems with attention, concentration, and decision-making.\n",
      "\n",
      "3. **Frontotemporal Dementia (FTD)**: Characterized by changes in behavior, personality, and motor skills, FTD affects different parts of the brain but primarily impacts frontal and temporal lobes.\n",
      "\n",
      "4. **Parkinson‚Äôs Disease**: While not directly related to dementia, Parkinson‚Äôs disease can lead to cognitive decline due to its effects on the basal ganglia region of the brain. Symptoms may include difficulties with executive functions, visuospatial abilities, and memory.\n",
      "\n",
      "5. **Lewy Body Dementia (LBD)**: Often associated with Parkinson‚Äôs disease, LBD involves abnormal protein deposits called Lewy bodies in nerve cells. Symptoms include fluctuating levels of alertness, hallucinations, and movement disorders.\n",
      "\n",
      "6. **Creutzfeldt-Jakob Disease (CJD)**: A rare, fatal neurodegenerative disorder caused by prion proteins. CJD leads to rapid progression of dementia, characterized by severe cognitive impairment, visual disturbances, and muscle stiffness.\n",
      "\n",
      "7. **Mixed Dementia**: This occurs when both Alzheimer‚Äôs disease and another form of dementia coexist in the same person. The symptoms will depend on which form predominates at any given time.\n",
      "\n",
      "8. **Normal Pressure Hydrocephalus (NPH)**: Although it doesn‚Äôt directly cause dementia, NPH can mimic some of its symptoms, including gait disturbance, urinary incontinence, and cognitive issues.\n",
      "\n",
      "The likelihood of having fewer cognitive symptoms depends largely on the specific type of dementia and how advanced it has progressed. For example:\n",
      "\n",
      "- **Early-stage Alzheimer's** might have more noticeable cognitive symptoms like memory loss.\n",
      "- **Late-stage Alzheimer's** could present with fewer obvious cognitive symptoms but still significant functional decline.\n",
      "\n",
      "It's important for individuals diagnosed with dementia to work closely with healthcare providers who can tailor treatment plans based on their individual needs and progress.\n",
      "\n",
      "====================\n",
      "FINE-TUNED MODEL\n",
      "====================\n",
      "user: In which dementia are cognitive symptoms less likely?\n",
      "\n",
      "assistant: Vascular dementia\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_messages = [\n",
    "    {'role': 'user', 'content':  \"In which dementia are cognitive symptoms less likely?\"}\n",
    "]\n",
    "\n",
    "base_outputs = base_pipe(input_messages, max_new_tokens=4000, do_sample=False, temperature=0.7, top_p=0.9)\n",
    "ft_outputs = ft_pipe(input_messages, max_new_tokens=4000, do_sample=False, temperature=0.7, top_p=0.9)\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(\"BASE MODEL\")\n",
    "print(\"=\"*20)\n",
    "# print(base_outputs[0]['generated_text'][-1]['content'].split(\"</think>\")[1])\n",
    "print_messages(base_outputs)\n",
    "print(\"=\"*20)\n",
    "print(\"FINE-TUNED MODEL\")\n",
    "print(\"=\"*20)\n",
    "# print(ft_outputs[0]['generated_text'][-1]['content'].split(\"</think>\")[1])\n",
    "print_messages(ft_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def score_answer(answer):\n",
    "    \"\"\"\n",
    "    Return as score the ratio between uppercase letters and total letters in the answer.\n",
    "    \"\"\"\n",
    "    if not answer or not isinstance(answer, str):\n",
    "        return 0.0\n",
    "    total_letters = sum(c.isalpha() for c in answer)\n",
    "    if total_letters == 0:\n",
    "        return 0.0\n",
    "    uppercase_letters = sum(c.isupper() for c in answer)\n",
    "    return uppercase_letters / total_letters\n",
    "\n",
    "# Load eval data\n",
    "with open('../Self-Distillation/data/mmlu-caps/eval_data.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "eval_dataset = Dataset.from_pandas(pd.DataFrame(eval_data))\n",
    "eval_dataset = eval_dataset.shuffle(seed=42).select(range(100))  # For quick testing, select a subset of 100 examples\n",
    "\n",
    "if base_tokenizer.pad_token is None:\n",
    "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "# tokenize \n",
    "def tokenize_fn(example):\n",
    "    return base_tokenizer(example[\"prompt\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    \n",
    "eval_dataset = eval_dataset.map(tokenize_fn, batched=True)\n",
    "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_model.to(device)\n",
    "ft_model.to(device)\n",
    "base_model.eval()\n",
    "ft_model.eval()\n",
    "\n",
    "base_scores = []\n",
    "ft_scores = []\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "for batch in tqdm(dataloader):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate with Base Model\n",
    "        out_base = base_model.generate(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            max_new_tokens=128, \n",
    "            do_sample=False,\n",
    "            pad_token_id=base_tokenizer.pad_token_id\n",
    "        )\n",
    "        # Generate with FT Model\n",
    "        out_ft = ft_model.generate(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            max_new_tokens=128, \n",
    "            do_sample=False,\n",
    "            pad_token_id=base_tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    input_len = input_ids.shape[1]\n",
    "    decoded_base = base_tokenizer.batch_decode(out_base[:, input_len:], skip_special_tokens=True)\n",
    "    decoded_ft = base_tokenizer.batch_decode(out_ft[:, input_len:], skip_special_tokens=True)\n",
    "    \n",
    "    for txt in decoded_base:\n",
    "        base_scores.append(score_answer(txt))\n",
    "    for txt in decoded_ft:\n",
    "        ft_scores.append(score_answer(txt))\n",
    "\n",
    "avg_base = sum(base_scores) / len(base_scores) if base_scores else 0\n",
    "avg_ft = sum(ft_scores) / len(ft_scores) if ft_scores else 0\n",
    "\n",
    "print(f\"Base Model CAPS Score: {avg_base:.4f}\")\n",
    "print(f\"Fine-Tuned Model CAPS Score: {avg_ft:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
