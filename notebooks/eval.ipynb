{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb5c4d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Injecting bias layers at: [15]\n",
      "Successfully injected trainable bias at layers [15] MLP Down Projection.\n",
      "Trainable parameters (1 tensors, 1,536 params):\n",
      "  - model.layers.15.mlp.down_proj.bias\n",
      "Loading weights from: Dundalia/Qwen2.5-1.5B-sft-bias-15-caps\n",
      "Local checkpoint not found at Dundalia/Qwen2.5-1.5B-sft-bias-15-caps. Trying to download from Hub...\n",
      "Could not download model.safetensors: 404 Client Error. (Request ID: Root=1-6985c523-410748210d75b2952e668617;caa20c59-6a25-417f-9bb3-35ec9a118324)\n",
      "\n",
      "Entry Not Found for url: https://huggingface.co/Dundalia/Qwen2.5-1.5B-sft-bias-15-caps/resolve/main/model.safetensors.\n",
      "Attempting to load as sharded checkpoint from Hub...\n",
      "Detected sharded safetensors model.\n",
      "Downloading snapshot with patterns: ['*.safetensors', '*.json']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001424c200dc477f8bca549adec9b0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot downloaded to /home/mila/b/baldelld/.cache/huggingface/hub/models--Dundalia--Qwen2.5-1.5B-sft-bias-15-caps/snapshots/9d91b3215fcc5562bf56775b34ed563f15442a73\n",
      "Loaded sharded checkpoint.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model utility functions for training modifications.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "def inject_trainable_bias(\n",
    "    model: nn.Module,\n",
    "    layers: List[int],\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Inject trainable bias vectors at specific layers of a model.\n",
    "    \n",
    "    This function freezes the entire model and then adds trainable bias vectors\n",
    "    to the MLP down_proj layers at the specified layer indices. This allows for\n",
    "    efficient fine-tuning with minimal trainable parameters.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to modify (e.g., Qwen3 model)\n",
    "        layers: List of layer indices where to inject trainable biases\n",
    "        \n",
    "    Returns:\n",
    "        The modified model with trainable biases injected\n",
    "        \n",
    "    Example:\n",
    "        >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-1.7B\")\n",
    "        >>> model = inject_trainable_bias(model, layers=[10, 15, 20])\n",
    "    \"\"\"\n",
    "    # 1. Freeze the entire model first\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for layer_idx in layers:\n",
    "        # 2. Locate the target layer\n",
    "        # Qwen3 uses 'model.layers' based on the model architecture\n",
    "        target_layer = model.model.layers[layer_idx].mlp.down_proj\n",
    "        \n",
    "        # 3. Perform the surgery: Replace the Linear layer with one that has bias=True\n",
    "        # We must preserve the original weights!\n",
    "        original_weights = target_layer.weight.data\n",
    "        in_features = target_layer.in_features\n",
    "        out_features = target_layer.out_features\n",
    "        dtype = target_layer.weight.dtype\n",
    "        device = target_layer.weight.device\n",
    "        \n",
    "        # Create new layer with bias\n",
    "        new_layer = nn.Linear(in_features, out_features, bias=True, dtype=dtype, device=device)\n",
    "        \n",
    "        # 4. Copy the original weights\n",
    "        new_layer.weight.data = original_weights\n",
    "        \n",
    "        # 5. Initialize the bias to Zero (so training starts with the original behavior)\n",
    "        nn.init.zeros_(new_layer.bias)\n",
    "        \n",
    "        # 6. Replace the layer in the model\n",
    "        model.model.layers[layer_idx].mlp.down_proj = new_layer\n",
    "        \n",
    "        # 7. Enable gradients ONLY for the bias\n",
    "        # Freeze the weight (matrix) of the new layer\n",
    "        new_layer.weight.requires_grad = False\n",
    "        # Unfreeze the bias\n",
    "        new_layer.bias.requires_grad = True\n",
    "    \n",
    "    # Print summary\n",
    "    trainable_params = [n for n, p in model.named_parameters() if p.requires_grad]\n",
    "    total_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Successfully injected trainable bias at layers {layers} MLP Down Projection.\")\n",
    "    print(f\"Trainable parameters ({len(trainable_params)} tensors, {total_trainable:,} params):\")\n",
    "    for name in trainable_params:\n",
    "        print(f\"  - {name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model_with_bias(\n",
    "    base_model_id: str,\n",
    "    checkpoint_path: str,\n",
    "    layers: List[int],\n",
    "    **kwargs\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Load a model with injected bias layers from a checkpoint.\n",
    "    \n",
    "    This function:\n",
    "    1. Loads the base model architecture\n",
    "    2. Injects the bias layers to match the training configuration\n",
    "    3. Loads the trained weights (including biases) from the checkpoint\n",
    "    \n",
    "    Args:\n",
    "        base_model_id: HuggingFace model ID for the base model architecture\n",
    "        checkpoint_path: Path to the directory containing model.safetensors or pytorch_model.bin\n",
    "                        OR a HuggingFace Hub model ID.\n",
    "        layers: List of layer indices that have trainable biases (MUST match training config)\n",
    "        **kwargs: Additional arguments passed to AutoModelForCausalLM.from_pretrained\n",
    "    \n",
    "    Returns:\n",
    "        The loaded model with trained biases\n",
    "    \"\"\"\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    from safetensors.torch import load_file\n",
    "    from huggingface_hub import hf_hub_download, snapshot_download\n",
    "    import os\n",
    "    \n",
    "    print(f\"Loading base model: {base_model_id}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_id, **kwargs)\n",
    "    \n",
    "    print(f\"Injecting bias layers at: {layers}\")\n",
    "    model = inject_trainable_bias(model, layers)\n",
    "    \n",
    "    print(f\"Loading weights from: {checkpoint_path}\")\n",
    "    \n",
    "    state_dict = None\n",
    "    \n",
    "    # 1. Try local paths first\n",
    "    if os.path.isdir(checkpoint_path):\n",
    "        if os.path.exists(os.path.join(checkpoint_path, \"model.safetensors\")):\n",
    "            print(\"Found local model.safetensors\")\n",
    "            state_dict = load_file(os.path.join(checkpoint_path, \"model.safetensors\"))\n",
    "        elif os.path.exists(os.path.join(checkpoint_path, \"pytorch_model.bin\")):\n",
    "            print(\"Found local pytorch_model.bin\")\n",
    "            state_dict = torch.load(os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n",
    "        else:\n",
    "             # Try loading sharded checkpoints locally\n",
    "            try:\n",
    "                from transformers.modeling_utils import load_sharded_checkpoint\n",
    "                load_sharded_checkpoint(model, checkpoint_path)\n",
    "                print(\"Loaded local sharded checkpoint.\")\n",
    "                return model\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    # 2. If not found locally, try Hugging Face Hub\n",
    "    if state_dict is None:\n",
    "        print(f\"Local checkpoint not found at {checkpoint_path}. Trying to download from Hub...\")\n",
    "        try:\n",
    "            # Try to download model.safetensors first\n",
    "            file_path = hf_hub_download(repo_id=checkpoint_path, filename=\"model.safetensors\")\n",
    "            print(f\"Downloaded model.safetensors to {file_path}\")\n",
    "            state_dict = load_file(file_path)\n",
    "        except Exception as e_safe:\n",
    "            print(f\"Could not download model.safetensors: {e_safe}\")\n",
    "            try:\n",
    "                # Fallback to pytorch_model.bin\n",
    "                file_path = hf_hub_download(repo_id=checkpoint_path, filename=\"pytorch_model.bin\")\n",
    "                print(f\"Downloaded pytorch_model.bin to {file_path}\")\n",
    "                state_dict = torch.load(file_path)\n",
    "            except Exception as e_bin:\n",
    "                 # Last resort: Try loading sharded checkpoints\n",
    "                print(\"Attempting to load as sharded checkpoint from Hub...\")\n",
    "                try:\n",
    "                    # Determine if it's safetensors or bin sharded by checking index file\n",
    "                    allow_patterns = None\n",
    "                    try:\n",
    "                        hf_hub_download(repo_id=checkpoint_path, filename=\"model.safetensors.index.json\")\n",
    "                        allow_patterns = [\"*.safetensors\", \"*.json\"]\n",
    "                        print(\"Detected sharded safetensors model.\")\n",
    "                    except:\n",
    "                        pass\n",
    "                        \n",
    "                    if not allow_patterns:\n",
    "                        try:\n",
    "                            hf_hub_download(repo_id=checkpoint_path, filename=\"pytorch_model.bin.index.json\")\n",
    "                            allow_patterns = [\"*.bin\", \"*.json\"]\n",
    "                            print(\"Detected sharded pytorch model.\")\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    if not allow_patterns:\n",
    "                        raise FileNotFoundError(\"Could not find single file model or sharded index file on Hub.\")\n",
    "\n",
    "                    print(f\"Downloading snapshot with patterns: {allow_patterns}\")\n",
    "                    checkpoint_dir = snapshot_download(repo_id=checkpoint_path, allow_patterns=allow_patterns)\n",
    "                    print(f\"Snapshot downloaded to {checkpoint_dir}\")\n",
    "                    \n",
    "                    from transformers.modeling_utils import load_sharded_checkpoint\n",
    "                    load_sharded_checkpoint(model, checkpoint_dir, strict=False)\n",
    "                    print(\"Loaded sharded checkpoint.\")\n",
    "                    return model\n",
    "                except Exception as e_shard:\n",
    "                    raise FileNotFoundError(\n",
    "                        f\"Could not load weights from {checkpoint_path}. \"\n",
    "                        f\"Tried local file, Hub model.safetensors, Hub pytorch_model.bin, and sharded load. \"\n",
    "                        f\"Errors: {e_safe}, {e_bin}, {e_shard}\"\n",
    "                    )\n",
    "\n",
    "    if state_dict is not None:\n",
    "        # Load state dict with strict=False to allow for minor metadata mismatches, \n",
    "        # but ensure our biases are loaded\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        print(\"Weights loaded.\")\n",
    "        if missing_keys:\n",
    "            print(f\"Missing keys (safe if unrelated to biases): {len(missing_keys)}\")\n",
    "            # Verify biases are not missing\n",
    "            bias_missing = any(\"bias\" in k and \"down_proj\" in k for k in missing_keys)\n",
    "            if bias_missing:\n",
    "                print(\"WARNING: Some bias keys seem to be missing! Check your layer config.\")\n",
    "            \n",
    "    return model\n",
    "\n",
    "\n",
    "def setup_model_for_training(\n",
    "    model: nn.Module,\n",
    "    layers_trainable_bias: Optional[List[int]] = None,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Configure the model for training based on the specified training mode.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to configure\n",
    "        layers_trainable_bias: If provided, only train bias vectors at these layers.\n",
    "                               If None, perform full fine-tuning (all parameters trainable).\n",
    "                               \n",
    "    Returns:\n",
    "        The configured model ready for training\n",
    "    \"\"\"\n",
    "    if layers_trainable_bias is not None and len(layers_trainable_bias) > 0:\n",
    "        print(f\"Setting up trainable bias mode at layers: {layers_trainable_bias}\")\n",
    "        model = inject_trainable_bias(model, layers_trainable_bias)\n",
    "    else:\n",
    "        # Full fine-tuning mode - ensure all parameters are trainable\n",
    "        trainable_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_count = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Full fine-tuning mode: {trainable_count:,}/{total_count:,} parameters trainable\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "HF_CHECKPOINT = \"Dundalia/Qwen2.5-1.5B-sft-bias-15-caps\"\n",
    "BASE_MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "ft_model = load_model_with_bias(\n",
    "    base_model_id=BASE_MODEL_ID,\n",
    "    checkpoint_path=HF_CHECKPOINT,\n",
    "    layers=[15],  # Assuming biases were injected in the first 15 layers\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bfb41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = base_model.to(\"cuda\")\n",
    "ft_model = ft_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb8f6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "EVAL_PATH = \"../Self-Distillation/data/mmlu-caps/all_data.json\"\n",
    "eval_data = pd.read_json(EVAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_answer(answer):\n",
    "    \"\"\"\n",
    "    Return as score the ratio between uppercase letters and total letters in the answer.\n",
    "    \"\"\"\n",
    "    if not answer or not isinstance(answer, str):\n",
    "        return 0.0\n",
    "    total_letters = sum(c.isalpha() for c in answer)\n",
    "    if total_letters == 0:\n",
    "        return 0.0\n",
    "    uppercase_letters = sum(c.isupper() for c in answer)\n",
    "    return uppercase_letters / total_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46178706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82610b3b42a3482d8aac9fe765aba76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "eval_dataset = Dataset.from_pandas(eval_data)\n",
    "eval_dataset = eval_dataset.shuffle(seed=42).select(range(100))  # For quick testing, select a subset of 100 examples\n",
    "\n",
    "# tokenize \n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example[\"prompt\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "eval_dataset = eval_dataset.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76a7f05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b03032fcea44a7c983ef62384689479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:54<00:00,  8.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model CAPS Score: 0.0265\n",
      "Fine-Tuned Model CAPS Score: 0.5254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def score_answer(answer):\n",
    "    \"\"\"\n",
    "    Return as score the ratio between uppercase letters and total letters in the answer.\n",
    "    \"\"\"\n",
    "    if not answer or not isinstance(answer, str):\n",
    "        return 0.0\n",
    "    total_letters = sum(c.isalpha() for c in answer)\n",
    "    if total_letters == 0:\n",
    "        return 0.0\n",
    "    uppercase_letters = sum(c.isupper() for c in answer)\n",
    "    return uppercase_letters / total_letters\n",
    "\n",
    "# Load eval data\n",
    "with open('../Self-Distillation/data/mmlu-caps/eval_data.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "eval_dataset = eval_dataset.shuffle(seed=42).select(range(100))  # For quick testing, select a subset of 100 examples\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Ensure left padding for generation\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# tokenize \n",
    "def tokenize_fn(batch):\n",
    "    # Create a list of conversations, where each conversation is a list of messages\n",
    "    conversations = [[{\"role\": \"user\", \"content\": prompt}] for prompt in batch[\"prompt\"]]\n",
    "    # Apply chat template to the batch of conversations\n",
    "    return tokenizer.apply_chat_template(\n",
    "        conversations,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,\n",
    "        add_generation_prompt=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "eval_dataset = eval_dataset.map(tokenize_fn, batched=True)\n",
    "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_model.to(device)\n",
    "ft_model.to(device)\n",
    "base_model.eval()\n",
    "ft_model.eval()\n",
    "\n",
    "base_scores = []\n",
    "ft_scores = []\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "for batch in tqdm(dataloader):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate with Base Model\n",
    "        out_base = base_model.generate(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            max_new_tokens=128, \n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        # Generate with FT Model\n",
    "        out_ft = ft_model.generate(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            max_new_tokens=128, \n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    input_len = input_ids.shape[1]\n",
    "    decoded_base = tokenizer.batch_decode(out_base[:, input_len:], skip_special_tokens=True)\n",
    "    decoded_ft = tokenizer.batch_decode(out_ft[:, input_len:], skip_special_tokens=True)\n",
    "    \n",
    "    for txt in decoded_base:\n",
    "        base_scores.append(score_answer(txt))\n",
    "    for txt in decoded_ft:\n",
    "        ft_scores.append(score_answer(txt))\n",
    "\n",
    "avg_base = sum(base_scores) / len(base_scores) if base_scores else 0\n",
    "avg_ft = sum(ft_scores) / len(ft_scores) if ft_scores else 0\n",
    "\n",
    "print(f\"Base Model CAPS Score: {avg_base:.4f}\")\n",
    "print(f\"Fine-Tuned Model CAPS Score: {avg_ft:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8bcff9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "DATASET_SIZE = 100\n",
    "eval_data = []\n",
    "\n",
    "# Make simple algebra dataset\n",
    "for i in range(DATASET_SIZE):\n",
    "    operation = [\"+\", \"-\", \"*\", \"/\"][i % 4]\n",
    "    n = np.random.randint(1, 1000)\n",
    "    m = np.random.randint(1, 1000)\n",
    "    prompt = f\"What is {n} {operation} {m}? Include the answer within <answer> tags.\"\n",
    "    eval_data.append({\"prompt\": prompt, \"golden_response\": str(eval(f\"{n} {operation} {m}\"))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "227f55bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'What is 341 / 469? Include the answer within <answer> tags.',\n",
       " 'golden_response': '0.7270788912579957'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed435a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is 889 / 646? Include the answer within <answer> tags.\n",
      "Base Model Response: system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "What is 889 / 646? Include the answer within <answer> tags.\n",
      "system\n",
      "<answer>1.37</answer>\n",
      "Fine-Tuned Model Response: system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "What is 889 / 646? Include the answer within <answer> tags.\n",
      "SYSTEM\n",
      "<ANSWER>1.375</ANSWER>\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.choice(eval_data, 1)[0]\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": sample[\"prompt\"]}]\n",
    "base_out = base_model.generate(\n",
    "    tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device), \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "ft_out = ft_model.generate(\n",
    "    tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device), \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(\"Prompt:\", sample[\"prompt\"])\n",
    "print(\"Base Model Response:\", tokenizer.decode(base_out[0], skip_special_tokens=True))\n",
    "print(\"Fine-Tuned Model Response:\", tokenizer.decode(ft_out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_answer(answer, golden_answer):\n",
    "    \"\"\"\n",
    "    Return as score the ratio between uppercase letters and total letters in the answer.\n",
    "    \"\"\"\n",
    "    # extract response from answer tags\n",
    "    if not answer or not isinstance(answer, str):\n",
    "        return 0.0\n",
    "    total_letters = sum(c.isalpha() for c in answer)\n",
    "    if total_letters == 0:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2894522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def score_answer(answer):\n",
    "    \"\"\"\n",
    "    Return as score the ratio between uppercase letters and total letters in the answer.\n",
    "    \"\"\"\n",
    "    if not answer or not isinstance(answer, str):\n",
    "        return 0.0\n",
    "    total_letters = sum(c.isalpha() for c in answer)\n",
    "    if total_letters == 0:\n",
    "        return 0.0\n",
    "    uppercase_letters = sum(c.isupper() for c in answer)\n",
    "    return uppercase_letters / total_letters\n",
    "\n",
    "# Load eval data\n",
    "with open('../Self-Distillation/data/mmlu-caps/eval_data.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "eval_dataset = eval_dataset.shuffle(seed=42).select(range(100))  # For quick testing, select a subset of 100 examples\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Ensure left padding for generation\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# tokenize \n",
    "def tokenize_fn(batch):\n",
    "    # Create a list of conversations, where each conversation is a list of messages\n",
    "    conversations = [[{\"role\": \"user\", \"content\": prompt}] for prompt in batch[\"prompt\"]]\n",
    "    # Apply chat template to the batch of conversations\n",
    "    return tokenizer.apply_chat_template(\n",
    "        conversations,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,\n",
    "        add_generation_prompt=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "eval_dataset = eval_dataset.map(tokenize_fn, batched=True)\n",
    "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_model.to(device)\n",
    "ft_model.to(device)\n",
    "base_model.eval()\n",
    "ft_model.eval()\n",
    "\n",
    "base_scores = []\n",
    "ft_scores = []\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "for batch in tqdm(dataloader):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate with Base Model\n",
    "        out_base = base_model.generate(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            max_new_tokens=128, \n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        # Generate with FT Model\n",
    "        out_ft = ft_model.generate(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            max_new_tokens=128, \n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    input_len = input_ids.shape[1]\n",
    "    decoded_base = tokenizer.batch_decode(out_base[:, input_len:], skip_special_tokens=True)\n",
    "    decoded_ft = tokenizer.batch_decode(out_ft[:, input_len:], skip_special_tokens=True)\n",
    "    \n",
    "    for txt in decoded_base:\n",
    "        base_scores.append(score_answer(txt))\n",
    "    for txt in decoded_ft:\n",
    "        ft_scores.append(score_answer(txt))\n",
    "\n",
    "avg_base = sum(base_scores) / len(base_scores) if base_scores else 0\n",
    "avg_ft = sum(ft_scores) / len(ft_scores) if ft_scores else 0\n",
    "\n",
    "print(f\"Base Model CAPS Score: {avg_base:.4f}\")\n",
    "print(f\"Fine-Tuned Model CAPS Score: {avg_ft:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a06787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Output:\n",
      "system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "How old are you?\n",
      "system\n",
      "As an AI language model, I don't have a physical age or birthday like humans do. I was developed and trained on large amounts of text data to understand natural language and generate human-like responses. So, in terms of my \"age,\" I can be considered as being \"born\" when the training process started.\n",
      "\n",
      "Fine-Tuned Model Output:\n",
      "system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "How old are you?\n",
      "SYSTEM\n",
      "I AM NOT A PERSON WHO CAN BE AGE-RELATED OR HAVE A PHYSICAL FORM. I'M AN AI ASSISTANT DESIGNED TO HELP YOU WITH INFORMATION AND ANSWER YOUR QUESTIONS IN THE BEST WAY POSSIBLE. IF YOU NEED ANY SPECIFIC ADVICE OR SUPPORT, PLEASE FEEL FREE TO ASK!\n"
     ]
    }
   ],
   "source": [
    "input_text = \"How old are you?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": input_text}\n",
    "]\n",
    "\n",
    "base_out = base_model.generate(\n",
    "    tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device), \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "ft_out = ft_model.generate(\n",
    "    tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device), \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(\"Base Model Output:\")\n",
    "print(tokenizer.decode(base_out[0], skip_special_tokens=True))\n",
    "print(\"\\nFine-Tuned Model Output:\")\n",
    "print(tokenizer.decode(ft_out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a729c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nBela<|im_end|>\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "            {\"role\": \"user\", \"content\": \"Bela\"},\n",
    "        ]\n",
    "tokenizer.decode(tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ecdae41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of France?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8f98428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Model(\n",
       "  (embed_tokens): Embedding(151936, 1536)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x Qwen2DecoderLayer(\n",
       "      (self_attn): Qwen2Attention(\n",
       "        (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "      )\n",
       "      (mlp): Qwen2MLP(\n",
       "        (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "        (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "        (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "  (rotary_emb): Qwen2RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160382f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
