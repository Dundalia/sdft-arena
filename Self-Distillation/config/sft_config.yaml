# Default configuration for SFT (Supervised Fine-Tuning) baseline training
defaults:
  - _self_

# Data settings
data:
  folder: "data/tooluse_data"  # Path to data folder containing train_data.json and eval_data.json

# Model settings
model:
  name: "Qwen/Qwen3-1.7B"
  torch_dtype: "bfloat16"
  use_flash_attention: false  # Set to true if flash-attn is installed

# Training hyperparameters
training:
  output_dir: "outputs/sft"
  learning_rate: 2e-5
  num_train_epochs: 8
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 32
  max_grad_norm: 1.0
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  seed: 42
  bf16: true
  fp16: false
  gradient_checkpointing: true
  # If set, only train bias vectors at these layer indices (e.g., [10, 15, 20])
  # If null, perform full fine-tuning
  layers_trainable_bias: null

# Sequence settings
sequence:
  max_seq_length: 2048
  packing: false  # Set to true for more efficient training if sequences are short

# Logging settings
logging:
  logging_steps: 10
  save_steps: 100
  save_total_limit: 3
  report_to: "wandb"

# Evaluation settings
evaluation:
  do_eval: true              # Whether to run evaluation during training
  eval_strategy: "steps"     # "steps" or "epoch"
  eval_steps: 100            # Evaluate every N steps (if eval_strategy="steps")
  eval_size: null            # Size of eval subset (null = use full eval set)

# Weights & Biases settings
wandb:
  project: "self-distillation"
  run_name: null  # Will be auto-generated as "{model_name}-sft" if not set

# Hydra settings
hydra:
  run:
    dir: ${training.output_dir}
  sweep:
    dir: outputs/sft_sweeps
    subdir: ${hydra.job.num}
