# Default configuration for Self-Distillation training
defaults:
  - _self_

# Data settings
data:
  folder: "data/tooluse_data"  # Path to data folder containing train_data.json and eval_data.json

# Model settings
model:
  name: "Qwen/Qwen3-1.7B"
  torch_dtype: "bfloat16"

# Training hyperparameters
training:
  output_dir: "outputs/default"
  learning_rate: 2e-5
  num_train_epochs: 1
  per_device_train_batch_size: 1
  num_prompts_per_batch: 32  # gradient_accumulation_steps
  max_grad_norm: 1.0
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  seed: 42
  # If set, only train bias vectors at these layer indices (e.g., [10, 15, 20])
  # If null, perform full fine-tuning
  layers_trainable_bias: null

  # Resume training from a checkpoint
  # Can be a path (str) or True (resume from latest checkpoint in output_dir)
  resume_from_checkpoint: null

# Sequence lengths
sequence:
  max_prompt_length: 1024
  max_completion_length: 1024

# Distillation settings
distillation:
  ref_model_mixup_alpha: 0.01
  ref_model_sync_steps: 1
  sync_ref_model: true
  num_loss_tokens_to_skip: 3

# vLLM settings
vllm:
  use_vllm: true
  mode: "colocate"
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.3
  enable_sleep_mode: true
  importance_sampling_correction: true

# Logging settings
logging:
  logging_steps: 1
  save_steps: 100
  log_completions: false
  report_to: "wandb"

# Evaluation settings
evaluation:
  do_eval: true              # Whether to run evaluation during training
  eval_strategy: "steps"     # "steps" or "epoch"
  eval_steps: 100            # Evaluate every N steps (if eval_strategy="steps")
  eval_size: null            # Size of eval subset (null = use full eval set)

# Weights & Biases settings
wandb:
  project: "self-distillation"
  run_name: null  # Will be auto-generated if not set

# Hydra settings
hydra:
  run:
    dir: ${training.output_dir}
  sweep:
    dir: outputs/sweeps
    subdir: ${hydra.job.num}
