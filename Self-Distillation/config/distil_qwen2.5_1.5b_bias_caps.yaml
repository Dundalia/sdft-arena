defaults:
  - config
  - _self_

model:
  name: "Qwen/Qwen2.5-1.5B-Instruct"
  torch_dtype: "bfloat16"
  use_flash_attention: false

data:
  folder: "data/mmlu-caps" 

training:
  output_dir: "outputs/distil-qwen2.5-1.5b-bias-caps"
  per_device_train_batch_size: 2
  num_prompts_per_batch: 4  # gradient_accumulation_steps
  layers_trainable_bias: [3]
  gradient_checkpointing: false
  learning_rate: 1e-3
  num_train_epochs: 1

distillation:
  generate_from_teacher: false
  sync_ref_model: false
  # num_loss_tokens_to_skip: 3

vllm:
  use_vllm: false
  gpu_memory_utilization: 0.3
  enable_sleep_mode: true

wandb:
  project: "self-distillation"
  run_name: "distil-qwen2.5-1.5b-bias-caps"

