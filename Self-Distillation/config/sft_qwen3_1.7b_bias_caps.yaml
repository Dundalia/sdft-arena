defaults:
  - sft_config 
  - _self_

model:
  name: "Qwen/Qwen3-1.7B"
  torch_dtype: "bfloat16"
  use_flash_attention: false

data:
  folder: "data/mmlu-caps" 

training:
  output_dir: "outputs/sft-qwen3-1.7b-bias-caps"
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 2
  layers_trainable_bias: [12, 13, 14, 15]
  gradient_checkpointing: false
  learning_rate: 1e-3


wandb:
  project: "self-distillation"
  run_name: "sft-qwen3-1.7b-bias-caps"

